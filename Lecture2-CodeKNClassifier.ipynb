{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Honour code\n",
    "On my honour, I pledge that I have neither received nor provided improper assistamce in the completion of this programming assignment. Signed by KIthandi Lincoln Kasengi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning: Programming Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watch the following three videos: `Machine Learning Recipes 3 ~ 5`. In one jupyter notebook, do coding for KMClassifer in Machine Learning Recipes 5 Video. Post your work at Piazza by 24 hours before the next class. Have a reasonably good document of the program as you have seen in my lecture note. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Recipes #3 - What Makes a Good Feature?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Watch and follow instructor Josh Gordon](https://www.youtube.com/watch?v=N9fDIAflCMY&t=2s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Classifiers__ are only as good as the feature you provide. __Good features__ are informative, independent and simple. We will briefly discuss what makes a good feature and eventually provide some tips for selecting a good feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Feature Selection**\n",
    "\n",
    "Feature selection is also called variable selection or attribute selection. It is the automatic selection of attributes in your data (such as columns in tabular data) that are most relevant to the predictive modeling problem you are working on.\n",
    "Feature selection methods aid you in your mission to create an accurate predictive model. They help you by choosing features that will give you as good or better accuracy whilst requiring less data.\n",
    "\n",
    "**What makes a good feature?**\n",
    "\n",
    "Classifiers are only as good as the features you provide. When doing binary classification, it is very easy to decide a good feature between two different things. For example; if one is to write a classifier to tell the difference between two types of dogs, Greyhound and Labrador. Suppose you use two features; **dog's height** and **eye colour.** Greyhounds tend to be more inches taller that Labradors, but not always. Likewise, the colour of the dogs' eye may not clearly distinguish between Greyhounds and Labradors. So if we create a feature we have to consider how they look for various values in a population.\n",
    "\n",
    "__Good Features__\n",
    "- __Avoid useless features:__ They can hurt your classifier's accuracy. For example; using dog's eye colour to distinguish between Greyhounds and Labrdors.\n",
    "- __Avoid redudant feature:__ Independent features are best. For example; having heights in centimeters and height in  inches as two distinct features would not make any sense.\n",
    "- __Features should be easy to understand:__ This makes learning process easier. For example; Imagine you want to predict how many days it will take to mail a letter between two different cities. A great feature to use would be the distance between the cities in miles. A much worse pair of features to us would be the cities' locations given by their latitudes and longitudes. This is because learning the relationship latitudes and longitudes is much harder and will require many more examples in your training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dog population\n",
    "greyhounds = 500\n",
    "labs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give them height\n",
    "grey_height = 28 + 4 * np.random.randn(greyhounds)\n",
    "lab_height = 24 + 4 * np.random.randn(labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize in histogram\n",
    "plt.hist([grey_height, lab_height], stacked = True, color = ['r', 'b'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "### Machine Learning Recipes #4 - Let’s Write a Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Watch and follow instructor Josh Gordon](https://www.youtube.com/watch?v=84gqSbLcBFE&t=1s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this recipe, we’ll write a basic pipeline for supervised learning with just 12 lines of code. We'll talk about training and testing data, how to train the classifier, and how to call the predict method and use it to classify our testing data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training and Testing Data__\n",
    "\n",
    "Before deploying your model, you need to verify that the model works well. One approach is to partition our datasets into two sets; __training__ and __testing__ datasets.\n",
    "\n",
    "- __Training dataset:__  is the actual dataset that we use to train the model. The model sees and learns from this data.\n",
    "- __Test Dataset:__  is the data samples used to provide an unbiased evaluation of a final model fit on the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__STEP 1__: Import a dataset\n",
    "\n",
    "Let's import iris datasets into scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__STEP 2__: Call the features\n",
    "\n",
    "Let's call the features(samples) X and the labels Y. We use X and Y because a classifier is more of a function.\n",
    "\n",
    "           f(x) = y\n",
    "\n",
    "In the above function, y(output/labels) is a function of x(features/samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data # features\n",
    "Y = iris.target # labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__STEP 3__: Partition data to train and test\n",
    "\n",
    "X_train and Y_train are features/samples and labels for the train dataset, while X_test and Y_test are features/samples and labels for the test dataset respectively. The test_size is specified as 0.5. This means that half of the data will be ised in testing. For example; if you have 150 samples in iris dataset, 75 will be in train dataset and 75 will be used for test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition data to train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__STEP 4__: Create a classifier\n",
    "\n",
    "As earlier explained, a classifier is a function ```f(x) = y``` where f(x) represents features and y is label.\n",
    "There are two ways of creating a classifier.\n",
    "- From a decision tree classifier\n",
    "- From a KNN(K-Nearest Neighbors) classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   a) From a decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "my_classifier = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   b) Using K-Neighbour classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "my_classifier = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__STEP 5__: Train the classifier\n",
    "\n",
    "Train the classifier using training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__STEP 6__: Call predict\n",
    "\n",
    "Call the predict method and use it to classify our testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = my_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__STEP 7__: Calculate accuracy\n",
    "\n",
    "Calculate accuracy by comparing predicted labels to true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9466666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Entire code for Machine Learning Recipe #4 \n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data # features\n",
    "Y = iris.target # labels\n",
    "\n",
    "# partition data to train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = .5)\n",
    "\n",
    "#Create a Classifier\n",
    "#from sklearn import tree\n",
    "#my_classifier = tree.DecisionTreeClassifier()\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "my_classifier = KNeighborsClassifier()\n",
    "\n",
    "#Train the classifier and call predict\n",
    "my_classifier.fit(X_train, Y_train)\n",
    "predictions = my_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy by comparing predicted labels to true labels\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "-------------------------------------------\n",
    "### Machine Learning Recipes #5 - Writing Our First Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Watch and follow instructor Josh Gordon](https://www.youtube.com/watch?v=AoeEHqVSNOw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this recipe, We'll start with our code from episode #4 and comment out the classifier we imported. Then, we'll code up a simple replacement - using a scrappy version of k-Nearest Neighbors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__STEP 1__: Comment out imports\n",
    "\n",
    "Using the code in `machine learning recipe #4`, comment out KNN imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n"
     ]
    }
   ],
   "source": [
    "# Comment out imports\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data # features\n",
    "Y = iris.target # labels\n",
    "\n",
    "# partition data to train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = .5)\n",
    "\n",
    "#Create a Classifier\n",
    "#from sklearn import tree\n",
    "#my_classifier = tree.DecisionTreeClassifier()\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "my_classifier = KNeighborsClassifier()\n",
    "\n",
    "#Train the classifier and call predict\n",
    "my_classifier.fit(X_train, Y_train)\n",
    "predictions = my_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy by comparing predicted labels to true labels\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__STEP 2__: Implement a class\n",
    "\n",
    "Implement a class for our classifier. I call the class __ScrappyKNN()__. Next, change the pipeline to use the __ScrappyKNN()__ class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Implement a class for our Classifier\n",
    "# class ScrappyKNN(): # Class ScrappyKNN implemented\n",
    "from sklearn import datasets \n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.5)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "my_classifier = ScrappyKNN() # Change pipeline to use the ScrappyKNN() class\n",
    "\n",
    "my_classifier.fit(X_train, y_train)\n",
    "\n",
    "predicitions = my_classifier.predict (X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print (accuracy_score (y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__STEP 3__: Understand interface\n",
    "\n",
    "Let's see what methods we need to implement. Looking for the interface for a classifier, we see there are two we care about; __fit__, which does the training, and __predict__, which does the prediction. \n",
    "\n",
    "- __def fit ():__ Takes the features/samples and labels for the training set as input.\n",
    "- __def predict():__ Receives the features for our testing data and returns predictions to the labels as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Understand interface\n",
    "\n",
    "class ScrappyKNN():\n",
    "    def fit(self, X_train, y_train): # declare fit data\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X_test): # declare predict data\n",
    "        pass\n",
    "    \n",
    "from sklearn import datasets \n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.5)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "my_classifier = ScrappyKNN()\n",
    "\n",
    "my_classifier.fit(X_train, y_train)\n",
    "\n",
    "predicitions = my_classifier.predict (X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print (accuracy_score (y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__STEP 4__: Get Pipeline working\n",
    "\n",
    "To begin, let's write something simpler. We will write a random classifier. To start, add some codes to __fit()__ and __predict()__. Inside the predict(), remember we need to return a list of predictions because the parameter __X_test__ is usually a 2D array or a list of lists. Each row contains features for one testing example. Then randomly pick a label from the training data and append that to our predictions. \n",
    "\n",
    "Let's run our code and see how well it does, but remember we have three types of flowers in the iris dataset, so our accuracy should be about 33 percent. But when we started thos exercise, our accuracy was 97 percent, so let's see whether we can improve this by implementing K-Nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26666666666666666\n"
     ]
    }
   ],
   "source": [
    "# Get pipeline working\n",
    "\n",
    "import random\n",
    "\n",
    "class ScrappyKNN():\n",
    "    def fit(self, X_train, y_train): # declare fit data\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for row in X_test:\n",
    "            label = random.choice (self.y_train)\n",
    "            predictions.append(label)\n",
    "        return predictions\n",
    "    \n",
    "from sklearn import datasets \n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.5)\n",
    "\n",
    "my_classifier = ScrappyKNN()\n",
    "\n",
    "my_classifier.fit(X_train, y_train)\n",
    "\n",
    "predictions = my_classifier.predict (X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print (accuracy_score (y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__STEP 5__: Introducing K-NN\n",
    "\n",
    "Let's implement our classifier using K-Nearest neighbors. K-nearest neighbor classifier can hardly be simpler described. This is an old saying, which can be found in many languages and many cultures. It's also metnioned in other words in the Bible: __\"He who walks with wise men will be wise, but the companion of fools will suffer harm\" (Proverbs 13:20 )__\n",
    "\n",
    "The principle behind nearest neighbor classification consists in finding a predefined number, i.e. the __'k'__ - of training samples closest in distance to a new sample, which has to be classified. The label of the new sample will be defined from these neighbors. k-nearest neighbor classifiers have a fixed user defined constant for the number of neighbors which have to be determined.\n",
    "\n",
    "__Determining the neighbors__\n",
    "\n",
    "To determine the similarity between two instances, we need a distance function.In our example, we use the Euclidean distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__STEP 6__: Measure distance\n",
    "\n",
    "We can measure distance using Euclidean distance. __Euclidean distance__ is just a distance measure between a pair of samples __a__ and __b__ in an n-dimensional feature space. \n",
    "\n",
    "Let's define Euclidean distance in our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37333333333333335\n"
     ]
    }
   ],
   "source": [
    "# Using Euclidean distance\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def euc(a, b):\n",
    "        return distance.euclidean(a, b)\n",
    "    \n",
    "class ScrappyKNN():\n",
    "    def fit(self, X_train, y_train): # declare fit data\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for row in X_test:\n",
    "            label = random.choice (self.y_train)\n",
    "            predictions.append(label)\n",
    "        return predictions\n",
    "    \n",
    "from sklearn import datasets \n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.5)\n",
    "\n",
    "my_classifier = ScrappyKNN()\n",
    "\n",
    "my_classifier.fit(X_train, y_train)\n",
    "\n",
    "predictions = my_classifier.predict (X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print (accuracy_score (y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__STEP 7__: Implement nearest neighbor algorithm\n",
    "\n",
    "To make a prediction for a test point, we calculate the distance of all the training points, then we will predict the testing point as the same label as the closest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement K-NN algorithm \n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def euc(a, b):\n",
    "        return distance.euclidean(a, b)\n",
    "    \n",
    "class ScrappyKNN():\n",
    "    def fit(self, X_train, Y_train):\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for row in X_test:\n",
    "            label = self.closest(row)\n",
    "            predictions.append(label)\n",
    "        return predictions\n",
    "    \n",
    "    def closest(self, row):\n",
    "        best_dist = euc(row, self.X_train[0])\n",
    "        best_index = 0\n",
    "        for index, X_train_row in enumerate(self.X_train):\n",
    "            dist = euc(row, X_train_row)\n",
    "            if dist < best_dist:\n",
    "                best_dist = dist\n",
    "                best_index = index\n",
    "        return self.Y_train[best_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__STEP 8__: Run Pipeline\n",
    "\n",
    "Let's now run the pipeline. When you run it, you'll notice that the accuracy is over 90 percent and may vary ecause of the randomness in the train and test split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n"
     ]
    }
   ],
   "source": [
    "# Run the pipeline\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def euc(a, b):\n",
    "        return distance.euclidean(a, b)\n",
    "    \n",
    "class ScrappyKNN():\n",
    "    def fit(self, X_train, Y_train):\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for row in X_test:\n",
    "            label = self.closest(row)\n",
    "            predictions.append(label)\n",
    "        return predictions\n",
    "    \n",
    "    def closest(self, row):\n",
    "        best_dist = euc(row, self.X_train[0])\n",
    "        best_index = 0\n",
    "        for index, X_train_row in enumerate(self.X_train):\n",
    "            dist = euc(row, X_train_row)\n",
    "            if dist < best_dist:\n",
    "                best_dist = dist\n",
    "                best_index = index\n",
    "        return self.Y_train[best_index]\n",
    "    \n",
    "    from sklearn import datasets \n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.5)\n",
    "\n",
    "my_classifier = ScrappyKNN()\n",
    "\n",
    "my_classifier.fit(X_train, y_train)\n",
    "\n",
    "predictions = my_classifier.predict (X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print (accuracy_score (y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- __KNN (K nearest neighbor)__ is based on minimum distance (for example Euclidean distance) from the query instance to the training samples to determine the K-nearest neighbors. After you gather K nearest neighbors, you take simple majority of these K-nearest neighbors to be the prediction of the query instance.\n",
    "\n",
    "__Pros:__\n",
    " - relatively simple,\n",
    " - working well for some problems.\n",
    " \n",
    "__Cons:__\n",
    " - computationally intensive,\n",
    " - hard to represent relationships between features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "__For the commandments, “You shall not commit adultery, You shall not murder, You shall not steal, You shall not covet,” and any other commandment, are summed up in this word: “You shall love your neighbor as yourself.__ Romans 13:9\n",
    "\n",
    "----------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
