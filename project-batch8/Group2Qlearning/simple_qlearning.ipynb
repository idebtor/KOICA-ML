{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have 5 rooms in a building connected by doors as shown in the figure below.  We'll number each room 0 through 4.  The outside of the building can be thought of as one big room (5).  Notice that doors 1 and 4 lead into the building from room 5 (outside).\n",
    "For this example, we'd like to put an agent in any room, and from that room, go outside the building (this will be our target room). In other words, the goal room is number 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"A Simple Network Problem\"](rooms_house.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: http://mnemstudio.org/ai/path/images/modeling_environment_clip_image002a.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Graph Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Graph Representation](rooms_graph.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: http://mnemstudio.org/ai/path/images/map1a.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"A reward network\"](rewards.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: http://mnemstudio.org/ai/path/images/map2a.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Reward Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"Reward Matrix\"](reward_matrix.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: http://mnemstudio.org/ai/path/images/r_matrix1.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q Learning Algorithm, building the Q -Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Set the gamma parameter(learning rate), and  rewards in matrix R.\n",
    "\n",
    "2. Initialize matrix Q to zero.\n",
    "\n",
    "3. For each episode:\n",
    "\n",
    "    Select a random initial state.\n",
    "\n",
    "    Do While the goal state has not been reached.\n",
    "\n",
    "        * Select one among all possible actions for the current state.\n",
    "        ** Using this possible action, consider going to the next state.\n",
    "        *** Get maximum Q value for this next state based on all possible actions.\n",
    "        **** Compute: Q(state, action) = R(state, action) + Gamma * Max[Q(next state, all actions)]\n",
    "        ***** Set the next state as the current state.\n",
    "    End Do\n",
    "\n",
    "End For"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: http://mnemstudio.org/path-finding-q-learning-tutorial.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numpy is a package to support doing matrix multiplications\n",
    "import numpy as np\n",
    "#Pylab is a package that provides a set of utilities and interfaces to process laboratory data\n",
    "import pylab as plt\n",
    "#Networkx a package to support creating network graphs\n",
    "import networkx as nx\n",
    "\n",
    "#pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generating the network for the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map cell to cell, add circular cell to goal point\n",
    "points_list = [(0,4),(4,0),(4,3),(4,5),(3,4),(3,2),(3,1),(2,3),(1,3),(1,5),(5,1),(5,4),(5,5)]\n",
    "\n",
    "#setting the goal\n",
    "goal = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the network from the points list\n",
    "G=nx.Graph()\n",
    "G.add_edges_from(points_list)\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw_networkx_nodes(G,pos)\n",
    "nx.draw_networkx_edges(G,pos)\n",
    "nx.draw_networkx_labels(G,pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Set the gamma parameter, and environment rewards in matrix R.\n",
    "# set the learning rate (gamma)\n",
    "gamma = 0.8\n",
    "\n",
    "#initialize the reward matrix X\n",
    "MATRIX_SIZE = 6\n",
    "R = np.matrix(np.ones(shape=(MATRIX_SIZE, MATRIX_SIZE)))\n",
    "R *= -1\n",
    "\n",
    "#change the values to be 0 if it is a viable path and 100 if it is a goal path\n",
    "for point in points_list:\n",
    "    if point[1] == goal:\n",
    "        R[point] = 100\n",
    "    else:\n",
    "        R[point] = 0\n",
    "\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Initialize matrix Q to zero.\n",
    "#create the Q-learning matrix which will hold all the lessons learned\n",
    "Q = np.matrix(np.zeros([MATRIX_SIZE,MATRIX_SIZE]))\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating some helper functions\n",
    "\n",
    "# available_actions returns the possible movements from a given point\n",
    "def available_actions(state):\n",
    "    current_state_row = R[state,]\n",
    "    av_act = np.where(current_state_row >= 0)[1]\n",
    "    return av_act\n",
    "\n",
    "# sample_next_action chooses randomly the next move based on the list of possible moves\n",
    "def sample_next_action(available_actions_range):\n",
    "    next_action = int(np.random.choice(available_act,1))\n",
    "    return next_action\n",
    "\n",
    "#update updates the current state with the next move, updating the Q-learning matrix in the process\n",
    "# Q(state, action) = R(state, action) + Gamma * Max[Q(next state, all actions)]\n",
    "def update(current_state, action, gamma):\n",
    "    max_index = np.where(Q[action,] == np.max(Q[action,]))[1]\n",
    "\n",
    "    if max_index.shape[0] > 1:\n",
    "        max_index = int(np.random.choice(max_index, size = 1))\n",
    "    else:\n",
    "        max_index = int(max_index)\n",
    "    max_value = Q[action, max_index]\n",
    "\n",
    "    Q[current_state, action] = R[current_state, action] + gamma * max_value\n",
    "#     print('max_value', R[current_state, action] + gamma * max_value)\n",
    "\n",
    "    if (np.max(Q) > 0):\n",
    "        return(np.sum(Q/np.max(Q)*100))\n",
    "    else:\n",
    "        return (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 700 \n",
    "scores = []\n",
    "for i in range(episodes):\n",
    "    current_state = np.random.randint(0, int(Q.shape[0]))\n",
    "    available_act = available_actions(current_state)\n",
    "    action = sample_next_action(available_act)\n",
    "    score = update(current_state,action,gamma)\n",
    "    scores.append(score)\n",
    "\n",
    "\n",
    "print(\"Trained Q matrix:\")\n",
    "#Lets normalize Q (i.e.; converte into percentage) by dividing all non-zero entries by the highest number\n",
    "print(Q/np.max(Q)*100)\n",
    "\n",
    "#check the learning progress\n",
    "plt.plot(scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = 0\n",
    "current_state = initial_state\n",
    "steps = [current_state]\n",
    "\n",
    "while current_state != goal:\n",
    "\n",
    "    next_step_index = np.where(Q[current_state,]\n",
    "        == np.max(Q[current_state,]))[1]\n",
    "\n",
    "    if next_step_index.shape[0] > 1:\n",
    "        next_step_index = int(np.random.choice(next_step_index, size = 1))\n",
    "    else:\n",
    "        next_step_index = int(next_step_index)\n",
    "\n",
    "    steps.append(next_step_index)\n",
    "    current_state = next_step_index\n",
    "\n",
    "print(\"Most efficient path:\")\n",
    "print(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example and code was inspired on the following content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manuel Amunategui, \n",
    "video (  https://www.youtube.com/watch?v=nSxaG_Kjw_w )\n",
    "article ( https://www.viralml.com/video-content.html?v=nSxaG_Kjw_w )\n",
    "\n",
    "Mic \n",
    "article ( https://firsttimeprogrammer.blogspot.com/2016/09/getting-ai-smarter-with-q-learning.html )\n",
    "\n",
    "Unknown\n",
    "article ( http://mnemstudio.org/path-finding-q-learning-tutorial.htm )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
